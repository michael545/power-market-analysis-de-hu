{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition and Preprocessing\n",
    "\n",
    "This notebook handles the cleaning and preprocessing of raw power market data:\n",
    "- **EPEX Spot (DE)**: Day-ahead prices from EPEX with quarter-hour resolution \n",
    "- **HUPX (HU)**: Hungarian power exchange data with hourly resolution\n",
    "\n",
    "## Key preprocessing tasks:\n",
    "1. Load and inspect raw data files\n",
    "2. Standardize timestamp formats across datasets\n",
    "3. Clean column names and remove unnecessary columns\n",
    "4. Handle missing values and data quality issues\n",
    "5. Aggregate quarter-hour EPEX data to hourly resolution\n",
    "6. Create unified dataset structure\n",
    "7. Export cleaned data for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pd --v: 2.3.1\n",
      "NPY --v: 2.3.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 50)\n",
    "\n",
    "\n",
    "print(f\"Pd --v: {pd.__version__}\")\n",
    "print(f\"NPY --v: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = Path.cwd().parent\n",
    "data_raw_path = project_root / \"data\" / \"raw\"\n",
    "data_processed_path = project_root / \"data\" / \"processed\"\n",
    "\n",
    "data_processed_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "epex_file = data_raw_path / \"Day-ahead_prices_202507070000_202508040000_Hour_Quarterhour.csv\"\n",
    "hupx_file = data_raw_path / \"Labs_DAM_Aggregated_Trading_Data_20250802_190631.csv\"\n",
    "\n",
    "print(\"File paths:\")\n",
    "print(f\"EPEX (DE) data: {epex_file}\")\n",
    "print(f\"HUPX (HU) data: {hupx_file}\")\n",
    "print(f\"Processed data output: {data_processed_path}\")\n",
    "\n",
    "print(f\"\\nFile existence check:\")\n",
    "print(f\"EPEX file exists: {epex_file.exists()}\")\n",
    "print(f\"HUPX file exists: {hupx_file.exists()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "\n",
    "epex_raw = pd.read_csv(epex_file, sep=';')\n",
    "\n",
    "print(f\"EPEX data shape: {epex_raw.shape}\")\n",
    "print(f\"Columns ({len(epex_raw.columns)}):\")\n",
    "for i, col in enumerate(epex_raw.columns):\n",
    "    print(f\"  {i+1:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(epex_raw.head(3))\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(epex_raw.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HUPX (Hungarian) data\n",
    "print(\"=\" * 60)\n",
    "print(\"LOADING AND INSPECTING HUPX (HUNGARIAN) DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "hupx_raw = pd.read_csv(hupx_file)\n",
    "\n",
    "print(f\"HUPX data shape: {hupx_raw.shape}\")\n",
    "print(f\"Columns ({len(hupx_raw.columns)}):\")\n",
    "for i, col in enumerate(hupx_raw.columns):\n",
    "    print(f\"  {i+1}. {col}\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(hupx_raw.head(3))\n",
    "\n",
    "print(f\"\\nData types:\")\n",
    "print(hupx_raw.dtypes)\n",
    "\n",
    "print(f\"\\nUnique values in 'Status' column:\")\n",
    "print(hupx_raw['Status'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLEANING EPEX DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Extract relevant columns for Germany and Hungary\n",
    "epex_clean = epex_raw[['Start date', 'Germany/Luxembourg [€/MWh] Original resolutions', \n",
    "                       'Hungary [€/MWh] Original resolutions']].copy()\n",
    "\n",
    "# Rename columns for clarity\n",
    "epex_clean.columns = ['timestamp_start', 'de_price_epex', 'hu_price_epex']\n",
    "\n",
    "# Convert timestamp to datetime\n",
    "epex_clean['timestamp_start'] = pd.to_datetime(epex_clean['timestamp_start'])\n",
    "\n",
    "# Replace '-' with NaN and convert prices to numeric\n",
    "epex_clean['de_price_epex'] = pd.to_numeric(epex_clean['de_price_epex'], errors='coerce')\n",
    "epex_clean['hu_price_epex'] = pd.to_numeric(epex_clean['hu_price_epex'], errors='coerce')\n",
    "\n",
    "# Filter out rows where both prices are NaN (quarter-hour resolution artifacts)\n",
    "epex_clean = epex_clean.dropna(subset=['de_price_epex', 'hu_price_epex'], how='all')\n",
    "\n",
    "print(f\"EPEX cleaned data shape: {epex_clean.shape}\")\n",
    "print(f\"Date range: {epex_clean['timestamp_start'].min()} to {epex_clean['timestamp_start'].max()}\")\n",
    "print(f\"Missing values:\")\n",
    "print(epex_clean.isnull().sum())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(epex_clean.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"AGGREGATING EPEX DATA TO HOURLY RESOLUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create hour column for aggregation\n",
    "epex_clean['hour'] = epex_clean['timestamp_start'].dt.floor('H')\n",
    "\n",
    "# Aggregate quarter-hour data to hourly (using mean for price data)\n",
    "epex_hourly = epex_clean.groupby('hour').agg({\n",
    "    'de_price_epex': 'mean',\n",
    "    'hu_price_epex': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Rename hour column to timestamp for consistency\n",
    "epex_hourly.rename(columns={'hour': 'timestamp'}, inplace=True)\n",
    "\n",
    "print(f\"EPEX hourly data shape: {epex_hourly.shape}\")\n",
    "print(f\"Date range: {epex_hourly['timestamp'].min()} to {epex_hourly['timestamp'].max()}\")\n",
    "print(f\"Missing values:\")\n",
    "print(epex_hourly.isnull().sum())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(epex_hourly.head())\n",
    "\n",
    "# Check for time gaps\n",
    "time_diff = epex_hourly['timestamp'].diff().dropna()\n",
    "print(f\"\\nTime gaps check:\")\n",
    "print(f\"Standard interval: {time_diff.mode().iloc[0]}\")\n",
    "print(f\"Any non-standard intervals: {(time_diff != pd.Timedelta('1H')).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"CLEANING HUPX DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Clean HUPX data\n",
    "hupx_clean = hupx_raw.copy()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "columns_to_drop = ['Status', 'Baseload price', 'Traded volume']\n",
    "hupx_clean = hupx_clean.drop(columns=columns_to_drop)\n",
    "\n",
    "# Rename columns for consistency\n",
    "hupx_clean.columns = ['delivery_day', 'hour', 'hu_price_hupx']\n",
    "\n",
    "# Convert delivery day to datetime\n",
    "hupx_clean['delivery_day'] = pd.to_datetime(hupx_clean['delivery_day'])\n",
    "\n",
    "# Create proper timestamp by combining delivery day and hour\n",
    "# Note: HUPX uses hour 1-24, but we need 0-23 for proper datetime\n",
    "hupx_clean['hour_0based'] = hupx_clean['hour'] - 1\n",
    "hupx_clean['timestamp'] = hupx_clean['delivery_day'] + pd.to_timedelta(hupx_clean['hour_0based'], unit='h')\n",
    "\n",
    "# Keep only relevant columns\n",
    "hupx_clean = hupx_clean[['timestamp', 'hu_price_hupx']].copy()\n",
    "\n",
    "# Sort by timestamp\n",
    "hupx_clean = hupx_clean.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f\"HUPX cleaned data shape: {hupx_clean.shape}\")\n",
    "print(f\"Date range: {hupx_clean['timestamp'].min()} to {hupx_clean['timestamp'].max()}\")\n",
    "print(f\"Missing values:\")\n",
    "print(hupx_clean.isnull().sum())\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(hupx_clean.head())\n",
    "\n",
    "# Check for time gaps\n",
    "time_diff = hupx_clean['timestamp'].diff().dropna()\n",
    "print(f\"\\nTime gaps check:\")\n",
    "print(f\"Standard interval: {time_diff.mode().iloc[0]}\")\n",
    "print(f\"Any non-standard intervals: {(time_diff != pd.Timedelta('1H')).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"MERGING DATASETS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Merge EPEX and HUPX data on timestamp\n",
    "merged_data = pd.merge(epex_hourly, hupx_clean, on='timestamp', how='outer')\n",
    "\n",
    "# Sort by timestamp\n",
    "merged_data = merged_data.sort_values('timestamp').reset_index(drop=True)\n",
    "\n",
    "print(f\"Merged data shape: {merged_data.shape}\")\n",
    "print(f\"Date range: {merged_data['timestamp'].min()} to {merged_data['timestamp'].max()}\")\n",
    "print(f\"Missing values:\")\n",
    "print(merged_data.isnull().sum())\n",
    "\n",
    "# Check data coverage\n",
    "print(f\"\\nData coverage analysis:\")\n",
    "print(f\"EPEX DE prices available: {merged_data['de_price_epex'].notna().sum()} hours\")\n",
    "print(f\"EPEX HU prices available: {merged_data['hu_price_epex'].notna().sum()} hours\")\n",
    "print(f\"HUPX HU prices available: {merged_data['hu_price_hupx'].notna().sum()} hours\")\n",
    "print(f\"Total time periods: {len(merged_data)} hours\")\n",
    "\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(merged_data.head(10))\n",
    "\n",
    "print(f\"\\nLast few rows:\")\n",
    "print(merged_data.tail(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ADDING DERIVED COLUMNS FOR ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Add time-based columns\n",
    "merged_data['date'] = merged_data['timestamp'].dt.date\n",
    "merged_data['hour'] = merged_data['timestamp'].dt.hour\n",
    "merged_data['weekday'] = merged_data['timestamp'].dt.day_name()\n",
    "merged_data['week_number'] = merged_data['timestamp'].dt.isocalendar().week\n",
    "merged_data['month'] = merged_data['timestamp'].dt.month\n",
    "\n",
    "# Create primary HU price column (prefer HUPX over EPEX for Hungary)\n",
    "merged_data['hu_price_primary'] = merged_data['hu_price_hupx'].fillna(merged_data['hu_price_epex'])\n",
    "\n",
    "# Calculate price spreads\n",
    "merged_data['de_hu_spread'] = merged_data['de_price_epex'] - merged_data['hu_price_primary']\n",
    "merged_data['epex_hupx_hu_spread'] = merged_data['hu_price_epex'] - merged_data['hu_price_hupx']\n",
    "\n",
    "# Add data quality flags\n",
    "merged_data['has_de_price'] = merged_data['de_price_epex'].notna()\n",
    "merged_data['has_hu_hupx'] = merged_data['hu_price_hupx'].notna()\n",
    "merged_data['has_hu_epex'] = merged_data['hu_price_epex'].notna()\n",
    "\n",
    "print(f\"Enhanced data shape: {merged_data.shape}\")\n",
    "print(f\"Columns: {list(merged_data.columns)}\")\n",
    "\n",
    "# Show weeks covered\n",
    "weeks_covered = sorted(merged_data['week_number'].dropna().unique())\n",
    "print(f\"\\nWeeks covered: {weeks_covered}\")\n",
    "\n",
    "# Focus on weeks 28, 29, 30\n",
    "target_weeks = [28, 29, 30]\n",
    "target_data = merged_data[merged_data['week_number'].isin(target_weeks)].copy()\n",
    "\n",
    "print(f\"\\nTarget weeks (28, 29, 30) data shape: {target_data.shape}\")\n",
    "print(f\"Date range for target weeks: {target_data['timestamp'].min()} to {target_data['timestamp'].max()}\")\n",
    "\n",
    "print(f\"\\nData availability by week:\")\n",
    "for week in target_weeks:\n",
    "    week_data = target_data[target_data['week_number'] == week]\n",
    "    print(f\"Week {week}: {len(week_data)} hours, DE prices: {week_data['has_de_price'].sum()}, HU HUPX: {week_data['has_hu_hupx'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY REPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create summary statistics for target weeks\n",
    "print(\"SUMMARY STATISTICS FOR WEEKS 28, 29, 30:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "summary_stats = target_data[['de_price_epex', 'hu_price_hupx', 'hu_price_epex', \n",
    "                             'hu_price_primary', 'de_hu_spread']].describe()\n",
    "print(summary_stats)\n",
    "\n",
    "print(f\"\\nMISSING DATA ANALYSIS:\")\n",
    "print(\"=\" * 30)\n",
    "missing_analysis = target_data[['de_price_epex', 'hu_price_hupx', 'hu_price_epex']].isnull().sum()\n",
    "total_hours = len(target_data)\n",
    "print(f\"Total hours in target weeks: {total_hours}\")\n",
    "for col, missing_count in missing_analysis.items():\n",
    "    pct_missing = (missing_count / total_hours) * 100\n",
    "    print(f\"{col}: {missing_count} missing ({pct_missing:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDATA AVAILABILITY BY WEEK:\")\n",
    "print(\"=\" * 30)\n",
    "for week in target_weeks:\n",
    "    week_data = target_data[target_data['week_number'] == week]\n",
    "    total_week_hours = len(week_data)\n",
    "    de_available = week_data['de_price_epex'].notna().sum()\n",
    "    hu_hupx_available = week_data['hu_price_hupx'].notna().sum()\n",
    "    \n",
    "    print(f\"\\nWeek {week} ({total_week_hours} hours):\")\n",
    "    print(f\"  DE prices: {de_available}/{total_week_hours} ({de_available/total_week_hours*100:.1f}%)\")\n",
    "    print(f\"  HU HUPX:   {hu_hupx_available}/{total_week_hours} ({hu_hupx_available/total_week_hours*100:.1f}%)\")\n",
    "\n",
    "# Check for price anomalies\n",
    "print(f\"\\nPRICE ANOMALY CHECK:\")\n",
    "print(\"=\" * 25)\n",
    "# Negative prices\n",
    "neg_de = (target_data['de_price_epex'] < 0).sum()\n",
    "neg_hu = (target_data['hu_price_primary'] < 0).sum()\n",
    "print(f\"Negative DE prices: {neg_de}\")\n",
    "print(f\"Negative HU prices: {neg_hu}\")\n",
    "\n",
    "# Extremely high prices (>500 EUR/MWh)\n",
    "high_de = (target_data['de_price_epex'] > 500).sum()\n",
    "high_hu = (target_data['hu_price_primary'] > 500).sum()\n",
    "print(f\"Very high DE prices (>500): {high_de}\")\n",
    "print(f\"Very high HU prices (>500): {high_hu}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXPORTING CLEANED DATA\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Export full merged dataset\n",
    "full_output_file = data_processed_path / \"merged_spot_prices_full.csv\"\n",
    "merged_data.to_csv(full_output_file, index=False)\n",
    "print(f\"Full dataset exported to: {full_output_file}\")\n",
    "\n",
    "# Export target weeks dataset (28, 29, 30)\n",
    "target_output_file = data_processed_path / \"spot_prices_weeks_28_29_30.csv\"\n",
    "target_data.to_csv(target_output_file, index=False)\n",
    "print(f\"Target weeks dataset exported to: {target_output_file}\")\n",
    "\n",
    "# Export summary for quick analysis\n",
    "summary_data = target_data.groupby(['week_number', 'date']).agg({\n",
    "    'de_price_epex': ['mean', 'min', 'max'],\n",
    "    'hu_price_primary': ['mean', 'min', 'max'],\n",
    "    'de_hu_spread': ['mean', 'min', 'max'],\n",
    "    'has_de_price': 'sum',\n",
    "    'has_hu_hupx': 'sum'\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "summary_data.columns = ['_'.join(col).strip() for col in summary_data.columns]\n",
    "summary_data = summary_data.reset_index()\n",
    "\n",
    "summary_output_file = data_processed_path / \"daily_summary_weeks_28_29_30.csv\"\n",
    "summary_data.to_csv(summary_output_file, index=False)\n",
    "print(f\"Daily summary exported to: {summary_output_file}\")\n",
    "\n",
    "print(f\"\\nData processing completed successfully!\")\n",
    "print(f\"Files created:\")\n",
    "print(f\"  1. {full_output_file.name} - Full merged dataset ({len(merged_data)} hours)\")\n",
    "print(f\"  2. {target_output_file.name} - Target weeks only ({len(target_data)} hours)\")\n",
    "print(f\"  3. {summary_output_file.name} - Daily summaries ({len(summary_data)} days)\")\n",
    "\n",
    "# Quick preview of target data\n",
    "print(f\"\\nFINAL PREVIEW - WEEKS 28, 29, 30:\")\n",
    "print(\"=\" * 40)\n",
    "print(target_data[['timestamp', 'week_number', 'de_price_epex', 'hu_price_hupx', \n",
    "                   'hu_price_primary', 'de_hu_spread']].head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "power-market-analysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
